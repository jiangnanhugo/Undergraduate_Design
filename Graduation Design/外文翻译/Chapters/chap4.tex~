\chapter{实验与分析}
上述框架包含多个可以调节的参数：
\begin{itemize}
	\item 是否使用白化手段，
	\item 特征的数量$K$，
	\item 步幅的大小$s$，
	\item 感受野的大小$w$。
\end{itemize}
在本节中，我们将展示着这参数对系统性能影响的实验结果。首先，我们将会在CIFAR-10训练数据集上利用交叉验证手段来评估这些参数的影响。然后，我们利用各自的非监督式学习算法，按照上述实验得出的分析结论并配置其最佳参数（即，在我们最后的结果中，对所有的算法我们将使用相同的设置），并给出在CIFAR-10和NORB数据测试集上面的实验结果。

我们基本的测试步骤如下。对于3.1.2节中介绍的每一种非监督式学习算法，在不同的参数$K,s,w$下，我们将分别利用白化后的数据和原始数据进行训练单层的特征。接下来，如3.2.2节中描述，我们将会训练一个线性分类器并测试其在对照数据集（主要用于分析）和测试数据集（用于最后的结论）上的表现。
\section{可视化}
在我们展示分类结果之前，我们首先展示学习到的特征图像。由稀疏自动编码器，稀疏限制玻尔兹曼机，k-means聚类，高斯混合模型学习到的质心展示在图\ref{fig:cent}，其中感受野的大小为8个像素。众所周知，自动编码器和限制玻尔兹曼机能学习到类似于Gabor滤波器的滤波效果。我们可以用白化后的数据和原始数据分别获得该结果。然而，这些图片也说明了相似的结果也可以用聚类算法获得。特别地，虽然对原始聚类产生的执行与\cite{6,29}一致,但当我们对白化后的数据做聚类时，其效果远超过局部过滤器并十分类似于其他算法产生的结果。这样看来，对原始数据做白化处理，再使用聚类算法，我们可以轻易获得类似于其他算法的效果并且不需要调节参数。


\begin{figure}
\centering
	\includegraphics[width=0.48\linewidth]{./Pictures/cent-1.eps}
	\includegraphics[width=0.48\linewidth]{./Pictures/cent-2.eps}
	\includegraphics[width=0.48\linewidth]{./Pictures/cent-3.eps}
    \includegraphics[width=0.48\linewidth]{./Pictures/cent-4.eps}
\caption{随机初始化质心并在CIFAR-10图像数据集上面应用不同学习算法训练结果。}
\label{fig:cent}
\end{figure}

\section{白化的效果}
接下来我们开始调整横轴的参数并研究其影响效果，首先是研究白化手段的影响效果，由图\ref{fig:cent}可知，它能直接改变质心的位置。图\ref{fig:performance}展示了所有算法在不同的特征数量和是否使用白化手段下它的总体表现情况，其中固定步幅为1，感受野大小为6个像素。
\begin{figure}
\centering
\includegraphics[scale=0.7]{./Pictures/performance.eps}
\caption{插图中显示了特征提取过程，其中感受野大小为$w*w$，步幅为$s$。我们首先提取$w*w$大小间隔为$s$像素的的图像小块，然后将它们映射到$K$维特征向量，形成新的图像特征表示。这些向量接着被池化并被划分成四个象限，最后拼接成特征向量以便于分类工作。（为了清楚起见，我们取右边的图时步幅大于$w$,但实际上平均步幅大小均小于$w$）。}
\label{fig:performance}
\end{figure}

对于稀疏自动编码器和限制玻尔兹曼机，白化手段的效果不是很明显。当只使用100特征数量时，稀疏限制玻尔兹曼机的性能提升最明显，但当使用更多的特征数时，这种性能提升逐渐消失了。然而，对于聚类算法，我们发现白化手段是必不可少的预处理过程，因为聚类算法无法处理数据内部的相关性问题。

聚类算法已经成功应用在原始像素输入数据上面\cite{6,29}，但这些应用尚未使用白化处理的数据数据。我们的结果表明如果使用白化手段，会有效提高它们的性能表现。
\section{特征的数量}
我们在实验中采用特征数量分别为100，200，400，800，1200和1600来学习特征表示。图\ref{fig:stride}清楚地显示了增加特征数量后带来的效果：所有的算法都随着特征数量的增加而性能提高。

\begin{figure}
\centering
\includegraphics[scale=0.7]{./Pictures/stride.eps}
\caption{步幅的效果}
\label{fig:stride}
\end{figure}


出人意料的是，采用k-means聚类算法，``三角''激活函数和白化处理手段时系统的性能表现最好。特别令人关注的是，k-means聚类无需调节任何参数，不像其他的稀疏自动编码器和稀疏玻尔兹曼机，他们需要调节很多超参数才能达到最好的实验结果。
\section{步幅的影响}
在我们的框架中，``步幅''$s$指的是提取特征值时两个图像小块之间的距离（参见图\ref{fig:conv}）。通常，学习系统的步幅$s>1$，因为计算特征映射十分耗费计算资源。例如，稀疏编码要求对每一个图像小块求解优化问题的解，这样把步幅设置为1，显然是不合理的。那么我们想要了解为了妥协性能方面的因素，我们会牺牲多少系统性能（他们都有自己的特征映射函数，可以非常迅速地计算出他们的特征值）。在本实验中，我们设置特征数量为1600，感受野的大小为6像素，却改变步幅的大小：1，2，4和8像素。实验结果展示在图\ref{fig:recept}中（我们不做高斯混合模型的实验，因为这样的参数配置对它来说不合理）。

\begin{figure}
\centering
\includegraphics[scale=0.7]{./Pictures/recept.eps}
\caption{感受野尺寸的影响}
\label{fig:recept}
\end{figure}

图中显示随着步长的增大，性能有明显的下降。然而，变化的幅度十分惊人：即使步幅设置为$s=2$，我们损失了3\%的精确度。当$s=4$时我们损失了5\%。在比较算法间的差异时，这种误差更加明显。例如，我们将稀疏限制玻尔兹曼机算法的步幅设置为$s=2$它的性能表现与硬分配的k-means聚类算法步幅设置为$s=1$时相同――该算法是在非监督算法领域最简单的算法（显然比稀疏限制玻尔兹曼机算法简单很多）。
\section{感受野的大小}
最后，我们还评估了感受野的大小产生的影响。给定一个可扩展的算法，若是扩大感受野的大小，他能学习到更复杂的特征，能涵盖图像更大的区域。另一方面，随着空间维度地增加，我们的算法需要学习更多的特征，需要更多的数据作支撑。因此，在相同的数据量和相同的特征数量下，我们尚未确定这是一个有效的措施。在本实验中，我们设置感受野的大小为$6,8,12$像素。对于其他参数，我们使用的白化手段，并且步幅为1像素，质心数量为1600。

实验结果展示在图\cite{5}。总体来说，感受野大小为6像素时效果最好，12像素大小的感受野产生的效果比其余情况都要差。因此，如果我们还有空余的计算资源，我们的实验结果表明，最好能将它用来减少步幅$s$并增加学习特征的数量。

不幸的是，不像其他参数，感受野的尺寸需要采用交叉验证手段来做出明智的选择。我们的实验表明，感受野的尺寸较小时，它可以表现的很好。这一点尤其重要，特别是当我们降低输入尺寸以便使用更小的步幅和学习更多的特征，他们都能对结果有明显的提升。





\section{最后的分类结果}
实验表明，针对CIFAR-10数据集，采用白化手段，步幅设置为1像素，感受野设置为6像素，采用大量的特征能在上诉算法中获得最好的实验结果。我们使用上述参数配置，在整个CIFAR-10训练集上面完成全部流程，再用来训练支持向量机分类器并在标准的CIFAR-10测试数据集上面做测试。我们最后的结果展示在表格\ref{tab:cifar-performance}中，表格还包含了其他文献中的实验结果。很奇怪的是，当特征数量为1600时，k-means(三角)算法达到了非常高的性能(77.9\%)。在此基础之上，我们试图通过增加特征数来那个来获得更好的效果，果不其然，当特征数量为4000时，我们的实验精度为79.6\%。

基于上述分析，我们在NORB数据集（数据已归一化）上运行了所有的算法。我们使用CIFAR-10数据集上获得的参数设置，包括感受野的大小为6像素。实验结果总结在表格\ref{tab:norb-performance}中。其中，所有的算法都能实现非常高的性能。同样，K-means聚类算法并使用``三角''激活函数能实现最高的性能。当我们设置CIFAR数据集上的特征数量为4000时，我们的实验正确率达到了97.21\%。然而，我们注意到其他算法获得的结果十分相似，无论使用什么算法。这表明，系统性能主要的因素是网络架构而不是选择非监督式学习算法的不同。

最后，我们在STL-10数据集上运行了我们的系统。该数据集使用更高的分辨率(96*96)的图像，但训练样本很少(每个类别是由100个实例)，但却提供了大量未标记的训练集――从而迫使算法从图像中统计获得先验知识。我们对STL图像进行降采样(32*32像素)并使用与CIFAR数据集相同的实验系统。在这种情况下，其性能表现远比CIFAR上的小的已标记的数据集低很多：$51.5\%(\pm 1.73\%)$(若采用原始数据，$31.8\%(\pm 0.62\%)$)。这表明，当我们有大量标注训练集是像NORB和CIFAR数据集，这里提出的方法是最佳选择。
\begin{table}[!hpt]

\begin{center}
{
\caption{CIFAR-10 数据集上的测试识别准确率\label{tab:cifar-performance}}}
\begin{tabular}{|c|c|}
\hline
原始像素(\cite{13}介绍过)    & 37.3\%\\
3种因子化玻尔兹曼机(3层)\cite{24}  & 65.3\%\\
均值-方差玻尔兹曼机(3层)\cite{23}    & 71.0\%\\
改进的本地坐标编码\cite{33}  & 74.5\%\\
卷积深度信念网络(2层)\cite{14}    & \textbf{78.9}\%\\
\hline
稀疏自动编码器    		& 73.4\%\\
稀疏限制玻尔兹曼机 		& 73.4\%\\
K-means(硬)    			& 68.6\%\\
K-means(三角)  			& 77.9\%\\
K-means(三角，4000特征数量)  	& \textbf{79.6}\%\\
\hline

\end{tabular}
\end{center}
\end{table}


\begin{table}[!hpt]
\begin{center}
{
\caption{NORB 数据集上的测试识别准确率(和误差率)\label{tab:norb-performance}}}
\begin{tabular}{|c|c|}
\hline
算法 & 正确率(误差率)  \\
\hline
卷积神经网络\cite{16}    & 93.4\%(6.6\%)  \\
深度玻尔兹曼机\cite{26}  & 92.8\%(7.2\%)  \\
深度信念网络\cite{20}    & 95.0\%(5.0\%)  \\
(\cite{11}的最好的结果)  & 94.4\%(5.6\%)  \\
深度神经网络\cite{27}    & \textbf{97.13}\%(\textbf{2.87}\%)  \\
\hline
稀疏自动编码器    		& 96.9\%(3.1\%)  \\
稀疏限制玻尔兹曼机 		& 96.2\%(3.8\%)  \\
K-means(硬)    			& 96.9\%(3.1\%)  \\
K-means(三角)  			& 97.0\%(3.0\%)  \\
K-means(三角，4000特征数量)  	& \textbf{97.21}\%(\textbf{2.79}\%)  \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[!hpt]
\label{tab:stl-performance}
\begin{center}
{
\caption{STL-10 数据集上的测试识别准确率}}
\begin{tabular}{|c|c|c|c|}
\hline
算法 & 正确率(误差率)  \\
\hline
原始像素   & 31.8\%($\pm$ 0.62\%)  \\
K-means(三角，1600特征数量)  	& \textbf{51.5}\%($\pm$ \textbf{1.73}\%)  \\
\hline
\end{tabular}
\end{center}
\end{table}
